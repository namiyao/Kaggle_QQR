{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Baseline\n",
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 128\n",
    "VALID_SPLIT = 0.05\n",
    "RE_WEIGHT = True\n",
    "\n",
    "LSTM(256)\n",
    "parameters of LSTM: 570368\n",
    "parameters of Dense: 513\n",
    "weights.001-0.3333.hdf5\n",
    "475s - loss: 0.3051 - acc: 0.7766 - val_loss: 0.3333 - val_acc: 0.7743\n",
    "\n",
    "LSTM(128)\n",
    "parameters of LSTM: 219648\n",
    "parameters of Dense: 257\n",
    "weights.001-0.3358.hdf5\n",
    "389s - loss: 0.3218 - acc: 0.7598 - val_loss: 0.3358 - val_acc: 0.7611\n",
    "\n",
    "LSTM(64)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 129\n",
    "weights.002-0.3393.hdf5\n",
    "356s - loss: 0.3138 - acc: 0.7675 - val_loss: 0.3393 - val_acc: 0.7641\n",
    "\n",
    "LSTM(64)*2\n",
    "parameters of LSTM: 126464 = 93440+33024\n",
    "parameters of Dense: 129\n",
    "weights.003-0.3385.hdf5\n",
    "400s - loss: 0.2892 - acc: 0.7953 - val_loss: 0.3385 - val_acc: 0.7695\n",
    "\n",
    "LSTM(64)*2\n",
    "BATCH_SIZE: 256 ==> 128\n",
    "parameters of LSTM: 126464 = 93440+33024\n",
    "parameters of Dense: 129\n",
    "weights.002-0.3367.hdf5\n",
    "767s - loss: 0.2986 - acc: 0.7842 - val_loss: 0.3367 - val_acc: 0.7680\n",
    "\n",
    "LSTM(64)*3\n",
    "parameters of LSTM: 159,488 = 93440+33024+33024\n",
    "parameters of Dense: 129\n",
    "weights.002-0.3395.hdf5\n",
    "604s - loss: 0.3081 - acc: 0.7750 - val_loss: 0.3395 - val_acc: 0.7612\n",
    "\n",
    "BIDIRECT(LSTM(64))\n",
    "parameters of LSTM: 186,880\n",
    "parameters of Dense: 257\n",
    "weights.006-0.3417.hdf5\n",
    "193s - loss: 0.3007 - acc: 0.7826 - val_loss: 0.3417 - val_acc: 0.7639\n",
    "\n",
    "LSTM(64)\n",
    "BatchNormalization()\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 129\n",
    "weights.003-0.3443.hdf5\n",
    "205s - loss: 0.2958 - acc: 0.7875 - val_loss: 0.3443 - val_acc: 0.7702\n",
    "\n",
    "LSTM(64,dropout=0.5)\n",
    "Dropout(0.5)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 129\n",
    "weights.004-0.3344.hdf5\n",
    "198s - loss: 0.3097 - acc: 0.7715 - val_loss: 0.3344 - val_acc: 0.7508\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(128)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+129\n",
    "weights.002-0.2749.hdf5\n",
    "199s - loss: 0.2354 - acc: 0.8314 - val_loss: 0.2749 - val_acc: 0.8093\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+65\n",
    "weights.002-0.2769.hdf5\n",
    "196s - loss: 0.2383 - acc: 0.8285 - val_loss: 0.2769 - val_acc: 0.8106\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)\n",
    "Dropout(0.5)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+65\n",
    "weights.008-0.3096.hdf5\n",
    "200s - loss: 0.2776 - acc: 0.7945 - val_loss: 0.3096 - val_acc: 0.7730\n",
    "\n",
    "LSTM(64,dropout=0.5)\n",
    "DENSE(64)\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+65\n",
    "weights.002-0.2788.hdf5\n",
    "197s - loss: 0.2391 - acc: 0.8271 - val_loss: 0.2788 - val_acc: 0.8087\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)*2\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+4160+65\n",
    "weights.002-0.2733.hdf5\n",
    "198s - loss: 0.2340 - acc: 0.8354 - val_loss: 0.2733 - val_acc: 0.8190\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)*3\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+4160+4160+65\n",
    "weights.002-0.2723.hdf5\n",
    "203s - loss: 0.2364 - acc: 0.8329 - val_loss: 0.2723 - val_acc: 0.8171\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)\n",
    "lr 1e-3 ==>1e-2\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+65\n",
    "weights.005-0.2899.hdf5\n",
    "198s - loss: 0.2458 - acc: 0.8240 - val_loss: 0.2899 - val_acc: 0.7940\n",
    "\n",
    "LSTM(64)\n",
    "DENSE(64)\n",
    "lr 1e-3 ==>1e-4\n",
    "parameters of LSTM: 93440\n",
    "parameters of Dense: 8256+65\n",
    "weights.021-0.2894.hdf5\n",
    "194s - loss: 0.2383 - acc: 0.8289 - val_loss: 0.2894 - val_acc: 0.7917\n",
    "\n",
    "LSTM(128)*3\n",
    "DENSE(128)*3\n",
    "weights.001-0.2713.hdf5\n",
    "709s - loss: 0.2539 - acc: 0.8190 - val_loss: 0.2713 - val_acc: 0.8094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time, json, os, math, pickle, sys\n",
    "from string import punctuation\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import concatenate, Embedding, Dense, Input, Dropout, Bidirectional, LSTM, BatchNormalization, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "MODEL = 'Baseline'\n",
    "if os.getcwd().split('/')[-1] != MODEL:\n",
    "    print('WRONG MODEL DIR!!!')\n",
    "CHECKPOINT_DIR = './checkpoint/'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.mkdir(CHECKPOINT_DIR)\n",
    "LOG_DIR = './log/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "OUTPUT_DIR = './output/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "    \n",
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 256\n",
    "VALID_SPLIT = 0.05\n",
    "RE_WEIGHT = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "# VOCAB_SIZE = 10000\n",
    "\n",
    "\n",
    "def get_best_model(checkpoint_dir = CHECKPOINT_DIR):\n",
    "    files = glob.glob(checkpoint_dir+'*')\n",
    "    val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "    index = val_losses.index(min(val_losses))\n",
    "    print('Loading model from checkpoint file ' + files[index])\n",
    "    model = load_model(files[index])\n",
    "    model_name = files[index].split('/')[-1]\n",
    "    print('Loading model Done!')\n",
    "    return (model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n",
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "trainval_df = pd.read_csv(DATA_DIR+\"train.csv\")\n",
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "print(trainval_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check for any null values\n",
    "# inds = pd.isnull(trainval_df).any(1).nonzero()[0]\n",
    "# trainval_df.loc[inds]\n",
    "# inds = pd.isnull(test_df).any(1).nonzero()[0]\n",
    "# test_df.loc[inds]\n",
    "\n",
    "# # Add the string 'empty' to empty strings\n",
    "# trainval_df = trainval_df.fillna('empty')\n",
    "# test_df = test_df.fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "abbr_dict={\n",
    "    \"i'm\":\"i am\",\n",
    "    \"'re\":\" are\",\n",
    "    \"'s\":\" is\",\n",
    "    \"'ve\":\" have\",\n",
    "    \"'ll\":\" will\",\n",
    "    \"n't\":\" not\",\n",
    "}\n",
    "\n",
    "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "\n",
    "# stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "#               'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "#               'Is','If','While','This']\n",
    "# print('stop_words:', len(stop_words))\n",
    "\n",
    "# # nltk.download(\"stopwords\")\n",
    "# stop_words = stopwords.words('english')\n",
    "# print('stop_words:', len(stop_words))\n",
    "\n",
    "\n",
    "def text_to_wordlist(text, abbr_dict=None, remove_stop_words=False, stem_words=False):\n",
    "    \n",
    "    if isinstance(text,float):\n",
    "        # turn nan to empty string\n",
    "        text = \"\"\n",
    "    else:\n",
    "#         Convert words to lower case and split them\n",
    "#         text = text.lower()\n",
    "\n",
    "#         # abbreviation replace\n",
    "#         # Create a regular expression  from the dictionary keys\n",
    "#         regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, abbr_dict.keys())))\n",
    "#         # For each match, look-up corresponding value in dictionary\n",
    "#         text = regex.sub(lambda mo: abbr_dict[mo.string[mo.start():mo.end()]], text) \n",
    "\n",
    "        words = []\n",
    "        for space_separated_fragment in text.strip().split():\n",
    "            words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "        text = [w for w in words if w]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "#         Remove punctuation from text\n",
    "#         text = ''.join([c for c in text if c not in punctuation])\n",
    "\n",
    "        # Optionally, remove stop words\n",
    "        if remove_stop_words:\n",
    "            text = text.split()\n",
    "            text = [w for w in text if not w in stop_words]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        # Optionally, shorten words to their stems\n",
    "        if stem_words:\n",
    "            text = text.split()\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            stemmed_words = [stemmer.stem(word) for word in text]\n",
    "            text = \" \".join(stemmed_words)\n",
    "        \n",
    "    # Return a list of words\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "trainval_df['len1'] = trainval_df.apply(lambda row: len(row['question1_WL'].split()), axis=1)\n",
    "trainval_df['len2'] = trainval_df.apply(lambda row: len(row['question2_WL'].split()), axis=1)\n",
    "\n",
    "test_df['len1'] = test_df.apply(lambda row: len(row['question1_WL'].split()), axis=1)\n",
    "test_df['len2'] = test_df.apply(lambda row: len(row['question2_WL'].split()), axis=1)\n",
    "\n",
    "lengths = pd.concat([trainval_df['len1'],trainval_df['len2']], axis=0)\n",
    "print(lengths.describe())\n",
    "print(np.percentile(lengths, 99.0))\n",
    "print(np.percentile(lengths, 99.4))\n",
    "print(np.percentile(lengths, 99.5))\n",
    "print(np.percentile(lengths, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file trainval_df.pickle\n",
      "Loading from file test_df.pickle\n"
     ]
    }
   ],
   "source": [
    "# question to word list by data cleaning\n",
    "\n",
    "file_name = 'trainval_df.pickle'\n",
    "if os.path.exists(OUTPUT_DIR+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    trainval_df = pd.read_pickle(OUTPUT_DIR+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)  \n",
    "    trainval_df['question1_WL'] = trainval_df.apply(lambda row: text_to_wordlist(row['question1']), axis=1)\n",
    "    trainval_df['question2_WL'] = trainval_df.apply(lambda row: text_to_wordlist(row['question2']), axis=1)\n",
    "    trainval_df.to_pickle(OUTPUT_DIR+file_name)      \n",
    "\n",
    "file_name = 'test_df.pickle'\n",
    "if os.path.exists(OUTPUT_DIR+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    test_df = pd.read_pickle(OUTPUT_DIR+file_name)\n",
    "else:\n",
    "    print ('Generating file '+file_name)  \n",
    "    test_df['question1_WL'] = test_df.apply(lambda row: text_to_wordlist(row['question1']), axis=1)\n",
    "    test_df['question2_WL'] = test_df.apply(lambda row: text_to_wordlist(row['question2']), axis=1)\n",
    "    test_df.to_pickle(OUTPUT_DIR+file_name)   \n",
    "    \n",
    "test_size = trainval_df.shape[0]-int(math.ceil(trainval_df.shape[0]*(1-VALID_SPLIT)/1024)*1024)\n",
    "train_df, valid_df = train_test_split(trainval_df, test_size=test_size, random_state=1986, stratify=trainval_df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tokenize and pad\n",
    "\n",
    "all_questions = pd.concat([trainval_df['question1_WL'],trainval_df['question2_WL'],test_df['question1_WL'],test_df['question2_WL']], axis=0)\n",
    "tokenizer = Tokenizer(num_words=None, lower=True)\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = len(word_index)\n",
    "print(\"Words in index: %d\" % nb_words) #126355\n",
    "\n",
    "train_q1 = pad_sequences(tokenizer.texts_to_sequences(train_df['question1_WL']), maxlen = MAX_LEN)\n",
    "train_q2 = pad_sequences(tokenizer.texts_to_sequences(train_df['question2_WL']), maxlen = MAX_LEN)\n",
    "valid_q1 = pad_sequences(tokenizer.texts_to_sequences(valid_df['question1_WL']), maxlen = MAX_LEN)\n",
    "valid_q2 = pad_sequences(tokenizer.texts_to_sequences(valid_df['question2_WL']), maxlen = MAX_LEN)\n",
    "y_train = train_df.is_duplicate.values\n",
    "y_valid = valid_df.is_duplicate.values\n",
    "\n",
    "train_q1_Double = np.vstack((train_q1, train_q2))\n",
    "train_q2_Double = np.vstack((train_q2, train_q1))\n",
    "valid_q1_Double = np.vstack((valid_q1, valid_q2))\n",
    "valid_q2_Double = np.vstack((valid_q2, valid_q1))\n",
    "y_train_Double = np.hstack((y_train, y_train))\n",
    "y_valid_Double = np.hstack((y_valid, y_valid))\n",
    "\n",
    "val_sample_weights = np.ones(len(y_valid_Double))\n",
    "if RE_WEIGHT:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    val_sample_weights *= 0.472001959\n",
    "    val_sample_weights[y_valid_Double==0] = 1.309028344\n",
    "else:\n",
    "    class_weight = None\n",
    "    val_sample_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file word_embedding_matrix.pickle\n"
     ]
    }
   ],
   "source": [
    "# load word_embedding_matrix\n",
    "\n",
    "file_name = 'word_embedding_matrix.pickle'\n",
    "if os.path.exists(OUTPUT_DIR+file_name):\n",
    "    print ('Loading from file '+file_name)\n",
    "    with open(OUTPUT_DIR+file_name, 'rb') as f:\n",
    "        word_embedding_matrix = pickle.load(f)\n",
    "else:\n",
    "    print ('Generating file '+file_name)   \n",
    "    # Load GloVe to use pretrained vectors\n",
    "    embeddings_index = {}\n",
    "    with open(DATA_DIR+'/glove/glove.840B.300d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings_index)) #1,505,774\n",
    "\n",
    "    # Need to use EMBEDDING_DIM for embedding dimensions to match GloVe's vectors.\n",
    "    nb_words = len(word_index)\n",
    "    null_embedding_words = []\n",
    "    word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_embedding_words.append(word)\n",
    "    print('Null word embeddings: %d' %len(null_embedding_words)) #43,229\n",
    "\n",
    "    with open(OUTPUT_DIR+file_name, 'wb') as f:\n",
    "        pickle.dump(word_embedding_matrix, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "null_embedding_word_counts = { word: word_counts[word] for word in null_embedding_words }\n",
    "print(sum(null_embedding_word_counts.values())) #454210\n",
    "\n",
    "word_docs = tokenizer.word_docs\n",
    "null_embedding_word_docs = { word: word_docs[word] for word in null_embedding_words }\n",
    "print(sum(null_embedding_word_docs.values())) #446584\n",
    "# 446584/(404290+2345796)/2 = 0.08119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RNNCELL_SIZE = 128\n",
    "RNNCELL_LAYERS = 3\n",
    "RNNCELL_DROPOUT = 0\n",
    "RNNCELL_RECURRENT_DROPOUT = 0\n",
    "RNNCELL_BIDIRECT = False\n",
    "DENSE_SIZE = 128\n",
    "DENSE_LAYERS = 3\n",
    "DENSE_DROPOUT = 0\n",
    "\n",
    "encode_model = Sequential()\n",
    "encode_model.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length=MAX_LEN, trainable=False))\n",
    "if RNNCELL_BIDIRECT:\n",
    "    for i in range(RNNCELL_LAYERS-1):\n",
    "        encode_model.add(Bidirectional(LSTM(RNNCELL_SIZE, dropout=RNNCELL_DROPOUT, recurrent_dropout=RNNCELL_RECURRENT_DROPOUT, \n",
    "                                            unroll=True, implementation=2, return_sequences=True)))\n",
    "    encode_model.add(Bidirectional(LSTM(RNNCELL_SIZE, dropout=RNNCELL_DROPOUT, recurrent_dropout=RNNCELL_RECURRENT_DROPOUT, \n",
    "                                        unroll=True, implementation=2)))\n",
    "else:\n",
    "    for i in range(RNNCELL_LAYERS-1):\n",
    "        encode_model.add(LSTM(RNNCELL_SIZE, dropout=RNNCELL_DROPOUT, recurrent_dropout=RNNCELL_RECURRENT_DROPOUT, \n",
    "                              unroll=True, implementation=2, return_sequences=True))\n",
    "    encode_model.add(LSTM(RNNCELL_SIZE, dropout=RNNCELL_DROPOUT, recurrent_dropout=RNNCELL_RECURRENT_DROPOUT, \n",
    "                          unroll=True, implementation=2))\n",
    "\n",
    "sequence1_input = Input(shape=(MAX_LEN,), name='q1')\n",
    "sequence2_input = Input(shape=(MAX_LEN,), name='q2')\n",
    "encoded_1 = encode_model(sequence1_input)\n",
    "encoded_2 = encode_model(sequence2_input)\n",
    "merged = concatenate([encoded_1, encoded_2], axis=-1)\n",
    "merged = Dropout(DENSE_DROPOUT)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "for i in range(DENSE_LAYERS):\n",
    "    merged = Dense(DENSE_SIZE, activation='relu', kernel_initializer='he_normal')(merged)\n",
    "    merged = Dropout(DENSE_DROPOUT)(merged)\n",
    "predictions = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[sequence1_input, sequence2_input], outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 40, 300)           37906800  \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 40, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 40, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 128)               131584    \n",
      "=================================================================\n",
      "Total params: 38,389,616.0\n",
      "Trainable params: 482,816.0\n",
      "Non-trainable params: 37,906,800.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "q1 (InputLayer)                  (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "q2 (InputLayer)                  (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)       (None, 128)           38389616                                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 128)           32896                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 128)           16512                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_34 (Dense)                 (None, 128)           16512                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)             (None, 128)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 1)             129                                          \n",
      "====================================================================================================\n",
      "Total params: 38,455,665.0\n",
      "Trainable params: 548,865.0\n",
      "Non-trainable params: 37,906,800.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 256\n",
      "Train on 770048 samples, validate on 38532 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 0.29254, saving model to ./checkpoint/weights.000-0.2925.hdf5\n",
      "734s - loss: 0.3268 - acc: 0.7411 - val_loss: 0.2925 - val_acc: 0.7875\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 0.29254 to 0.27127, saving model to ./checkpoint/weights.001-0.2713.hdf5\n",
      "709s - loss: 0.2539 - acc: 0.8190 - val_loss: 0.2713 - val_acc: 0.8094\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss did not improve\n",
      "708s - loss: 0.2088 - acc: 0.8615 - val_loss: 0.2806 - val_acc: 0.8292\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss did not improve\n",
      "708s - loss: 0.1705 - acc: 0.8929 - val_loss: 0.3256 - val_acc: 0.8363\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-c844293a0ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'q1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_q1_Double\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_q2_Double\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_Double\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           shuffle=True, class_weight=class_weight, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1),\n",
    "             EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1),\n",
    "             ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True),\n",
    "             TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=False, write_images=True)]\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "model.fit({'q1': train_q1_Double, 'q2': train_q2_Double}, y_train_Double, \n",
    "          batch_size=BATCH_SIZE, epochs=100, verbose=2, callbacks=callbacks, \n",
    "          validation_data=({'q1': valid_q1_Double, 'q2': valid_q2_Double}, y_valid_Double, val_sample_weights), \n",
    "          shuffle=True, class_weight=class_weight, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#resume training\n",
    "\n",
    "model, model_name = get_best_model()\n",
    "# model = load_model(CHECKPOINT_DIR + 'weights.025-0.4508.hdf5')\n",
    "# model_name = 'weights.025-0.4508.hdf5'\n",
    "# print('model_name', model_name)\n",
    "\n",
    "# #try increasing learningrate\n",
    "# optimizer = Adam(lr=1e-4)\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1),\n",
    "#              EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1),\n",
    "#              ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True),\n",
    "#              TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=False, write_images=True)]\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "model.fit({'q1': train_q1_Double, 'q2': train_q2_Double}, y_train_Double, \n",
    "          batch_size=BATCH_SIZE, epochs=100, verbose=2, callbacks=callbacks, \n",
    "          validation_data=({'q1': valid_q1_Double, 'q2': valid_q2_Double}, y_valid_Double, val_sample_weights), \n",
    "          shuffle=True, class_weight=class_weight, initial_epoch=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = load_model(CHECKPOINT_DIR + 'weights.001-0.2713.hdf5')\n",
    "model_name = 'weights.001-0.2713.hdf5'\n",
    "print('model_name', model_name)\n",
    "val_loss = model.evaluate({'q1': valid_q1_Double, 'q2': valid_q2_Double}, y_valid_Double, sample_weight=val_sample_weights, batch_size=BATCH_SIZE, verbose=2)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create submission\n",
    "test_q1 = pad_sequences(tokenizer.texts_to_sequences(test_df['question1_WL']), maxlen = MAX_LEN)\n",
    "test_q2 = pad_sequences(tokenizer.texts_to_sequences(test_df['question2_WL']), maxlen = MAX_LEN)\n",
    "predictions = model.predict({'q1': test_q1, 'q2': test_q2}, batch_size=BATCH_SIZE, verbose=2)\n",
    "predictions += model.predict({'q1': test_q2, 'q2': test_q1}, batch_size=BATCH_SIZE, verbose=2)\n",
    "predictions /= 2\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
    "submission.insert(0, 'test_id', test_df.test_id)\n",
    "file_name = MODEL+'_'+model_name+'_LSTM{:d}*{:d}_DENSE{:d}*{:d}_valloss{:4f}.csv' \\\n",
    ".format(RNNCELL_SIZE,RNNCELL_LAYERS,DENSE_SIZE,DENSE_LAYERS,val_loss)\n",
    "submission.to_csv(OUTPUT_DIR+file_name, index=False)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sys.stdout = open(OUTPUT_DIR+'training_output.txt', 'a')\n",
    "history = model.fit({'q1': train_q1, 'q2': train_q2}, y_train, batch_size=BATCH_SIZE, epochs=3, verbose=2, callbacks=callbacks, \n",
    "                    validation_data=({'q1': valid_q1, 'q2': valid_q2}, y_valid), shuffle=True, initial_epoch=0)\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                              'train_acc': history.history['acc'],\n",
    "                              'valid_acc': history.history['val_acc'],\n",
    "                              'train_loss': history.history['loss'],\n",
    "                              'valid_loss': history.history['val_loss']})\n",
    "summary_stats\n",
    "\n",
    "plt.plot(summary_stats.train_loss) # blue\n",
    "plt.plot(summary_stats.valid_loss) # green\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "units = 128 # Number of nodes in the Dense layers\n",
    "dropout = 0.25 # Percentage of nodes to drop\n",
    "nb_filter = 32 # Number of filters to use in Convolution1D\n",
    "filter_length = 3 # Length of filter for Convolution1D\n",
    "# Initialize weights and biases for the Dense layers\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "bias = bias_initializer='zeros'\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length = MAX_LEN, trainable = False))\n",
    "model1.add(Convolution1D(filters=nb_filter, kernel_size=filter_length, padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout))\n",
    "model1.add(Convolution1D(filters=nb_filter, kernel_size=filter_length, padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout))\n",
    "model1.add(Flatten())\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length = MAX_LEN, trainable = False))\n",
    "model2.add(Convolution1D(filters=nb_filter, kernel_size=filter_length, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(dropout))\n",
    "model2.add(Convolution1D(filters=nb_filter, kernel_size=filter_length, padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(dropout))\n",
    "model2.add(Flatten())\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length = MAX_LEN, trainable = False))\n",
    "model3.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(dropout))\n",
    "model3.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length = MAX_LEN, trainable = False))\n",
    "model4.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(dropout))\n",
    "model4.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, )))\n",
    "\n",
    "\n",
    "modela = Sequential()\n",
    "modela.add(Merge([model1, model2], mode='concat'))\n",
    "modela.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "modela.add(BatchNormalization())\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(dropout))\n",
    "modela.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "modela.add(BatchNormalization())\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "modelb = Sequential()\n",
    "modelb.add(Merge([model3, model4], mode='concat'))\n",
    "modelb.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "modelb.add(BatchNormalization())\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "modelb.add(BatchNormalization())\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([modela, modelb], mode='concat'))\n",
    "model.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
